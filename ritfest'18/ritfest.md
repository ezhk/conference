# Микросервисы на фронтенде в почте mail.ru

Основные проблемы в сложном проекте с онолитным репозиторием:

- трудно разобраться новому сотруднику
- внедрение новых технологий и пониженная скорость разработки
- регрессионные тесты ~1 день

Основные критерии:

- скорость разработки
- переносимый функционал
- проводить различные А/Б-тесты

Решили сделать через iframe и webview в ios/android, для разных платформ разные стили (платформа в get-запросе).
Т.о. отдельный проект получает 5 кратный прирост в тестировании по сравнению с основным проектом.
При этом не требуется деплоить каждый раз приложение с appstore/playmarket.

Проблемы iframe:

- двойной скролл:
  - руками указываем высоту — postmessaege с указанием высоты
  - set timeout
  - встроить iframe в iframe (это и использует mail.ru),
    чтобы iframe спроецировать в 100% высоты
- media query — здесь на помощь bundle.js
- с bundle.js по-прежнему осталась проблема «как легко поднимать новую версию»

Storybook — позволяет визуализировать различные этапы интерфейсов.
Легколсть в создании проектов — как project template с единой структурой всех слоев.
Чтобы избежать проблем с раскладкой без выкладки основного проекта:
используются удаленные и локальные конфиги с указанием версий включаемых приложений,
при выборе версий между удаленным и локальным конфигом используется та, что больше.

Kapellmeister — дирижер конфигов — на выходе json файл со всем версиями файлов для загрузки.
На основании версий пакетов строятся правильные URL-ы. С ним легко поднимать версии библиотек.

При переходе на микросервисы — исправили изначальные проблемы, плюс:

- смогли переиспользовать код на ios/android
- возможность быстро переписать проект


# Сетевые технологии в современных частных облаках

Облако в selectel на базе Openstack.

Проблемы облачных сетей:

- большое L2 сегменты (unicast flood убивает всю сеть)
- большое кол-во mac-адресов
- ограниченность VLAN (их всего 4096)
- соединение нескольких ДЦ
- гибридные сети (когда клиент хочет соединить
  в одну сеть свои виртуалки из разных сетей,
  e.g. своя сеть и aws)
- защита трафика

Подключение нескольких VM — виртуальный свитч:

- linux bridge: изучает mac, умеет learning/flooding, STP
- open vSwitch: аналочен linux bridge + LACT, туннели, openFlow, поддержка DPDK, multi-table fwd

Selectel рекомендует использовать openvswitch в качестве наиболее полного.

Чтобы объединить виртуалки на разных нодых:

- vlan — дорого (надо протянуть везде vlan)
- использование туннелей

Нужно разделять виртуалки разных клиентов:

- использование тега vlan ($ ovs-vsctl show)
- ограничить взаимодействие машин по mac-адресам (очень много правил)

Для отправки трафика в туннели используется mapping tag <-> VNI.

Для миграции виртуалок:

- использование центрального контроллера, подключающегося с open vSwitch
- установка agent-ов на ноды, и тут агенты взаимодействуют с cloud controller

В openstack схема с агентом — neutrib-ovs-agent.

Наиболее популярные протоколы:

- VXLAN
- GRE

потому что у низ есть bare-metal поддержка (то есть само железное
оборудование тоже должно уметь его, например Cisco)
GRE больше использует CPU time на обработку трафика по сранению с VXLAN.

Как отдать трафик в другую подсеть:

- выдача public IP на VM (но опять нужно растягивать vlan на разные ноды)
- у VM серая подсеть, транслируется на vRouter во внешний IP, и дальше на bare-metal router
- NAT-ом занимается специализированная железка (почти как предыдущий вариант,
  но контроллер должен уметь работать с этим железом)

vRouter:

- на базе VM
- linux namespace (отделяем таблицы маршрутизации и conntrack в отдельные namespace)

neutro-l3-agent — занимается настройкой L3-роутеров.
Failover: запускается два qrouter и между ними VRRP.

**На этом этапе решены первые 3 изначальные проблемы.**

**Использование нескольких ДЦ**
построение overlay сетей, для которой нужна всего l3 связность.
Как этим управлять:

- контроллер, который знает обо всех инсталляциях облака —
  дополнительная и очень критичная точка отказа
- cloud controller внутри ДЦ, а super controller управляет контроллером внутри ЦОД

**Гибридные сети**

- не имеет доступа на bare-metal клиента
- нельзя использовать VLAN, так как возвращаемся в кроблеме растяшивания
- нет VXLAN на bare-metal
- автоматизация

Решение проблемы — использование VTEP-свитча (VXLAN tunnel end point);
при этом cloud контроллер должен уметь им управлять.

Как работает наше облако с физической сетью:

- внедрение bgp для управления адресами между агентом на ноде и роутером, анонс /32,
  здесь нет ARP-а, а bgp-сигналинг
- миграция между VM с сохранением адреса

**Защита трафика**
Open vSwitch умеет шифрование, но сам openstack не умеет использовать эти методы в ovs.
Но уже сейчас есть VXLAN esp.

Проблемы производительности (и их варианты решения):

- стек linux tcp/ip (раскидывание очередей, использование буферов)
- tap интерфейсы
  Внедрение DPDK, open vSwitch умеет его.
  Но: ovs-dpdk при запуске с namespace DPDK каждый раз ходит в ядро (вместо первого матчинга),
  постоянно имеет contect switching; тут лучше работает обычный ovs с tcp-стеком
- сетевой драйвер в гостевой системе

**Развитие**

- SDN -> интеграция с bare-metal устройствами, если они не умеют openflow
- NFV (network function virtualization) -> vendor-image
- cloud networks -> BGP EVPN, шифрование
- linux netstack -> XDP eBPF
- open vswitch -> OVN
- NIC -> производительность с NFV
- phy net -> переход на 100Gb, IP-Fabric
  (использование MP-bgp для анонсирования mac-адресов,
  ограничение на 65к адресов — в зависимости от вендора)

2.5m PPS на одну ноду.


# Построение вычислительного облака на современном ландшафте технологий

Для реализации задач клиентов используют open stack
(понравилось, что можно добавлять метаданные внутрь железок)
и ceph для резервирования дисков.

kubernetes cluster — для быстрого деплоя.
Kafka — центральная шина данных и mesos cluster для обработки bigdata.
Mesos: есть удобные фрейморки для запуска web-версий приложений.

Правда cassandra пришлось выделить отдельно, поскольку на момент
разворачивания инфраструктуры не было готового фреймворка.

Инфрастуктура:

- grafana + prometeus
- ELK — сбор логов
- git — для хранения кода

Как сервис discovery — consul. Зашили в nova рецепты pupprt.
Consul оказался весьма прожорлив при большом количестве запросов и вирт. машин.

Единая точка входа — kafka, которую разбирает либо spark
(перекладывает сообщения в cassandra), либо flink.
Почему отказались от apache ignite — это больше кеш сервер, нежели брокер очередей.

Как протестировать платформу:

- отдельная пропускная способность элементов
- придумать цепочки обработки

Яндекс.танк не очень подошёл — очень много ammo-файлов
и уперлась производительность в ноду, а не в платформу.

Трейсинг пакетов:
предполагали использовать zipkin, но он существенно понижает пропускную способность,
поэтому решили trace ID засовывать внутрь пакетов и измерять свои timestamp.

Важно зафиксировать версии ПО, поскольку очень сильно изменяется производительность.


# Мониторинг и Kubernetes

[youtube](youtube.com/c/flant)
[github](github.com/flant/)

Мониторинг — это не графики и не алерты,
это система:

- упреждающая аварии,
- позволяет уведомить
- помогает осуществить быструю диагностику (хорошо собранные данные)

Эволюция развития деплоя:
железо > виртуалки (динамика x5) > kubernetes (x50)
С kubernetes:

- перестаем думать об отдельных контейнерах, смотрим только на группы;
- service discovery становится обязательным;
- вырос объем данных (pod-ов больше = данных больше);
- текучка метаданных (за год при частом деплое это могут сотни тысяч, миллионы данных,
  которые не нужно хранить — у объекта короткая жизнь).

У kubernetes есть namespace > deplyment > pods > apps/data;
на уровне железа эти все контейнеры как некоторое странное облако,
поэтому нам надо знать где какой под привязан к железу.
Что должно быть в мониторинге — всё перечисленное ns/deployment/pod (node).

Что делать? — Взять prometheus (написан на Go) и правильно настроить.
Почему:
prometheus наиболее популярен
  Heapster — хранит данные только последние 20 минут и только CPU и mem.
  cadvisor — источник данных для prometheus (тот же хипстер оттуда берет данные).
  А дальше datadog, new relic и sysdig — которые очень ниже по популярности.

Collector отправляет http-запрос pod-н и получает данные с метриками (название{label} value = my_metric{label="a"} t1=1 t2=5 t3=100);
далее отправляет в TSDB (timeseries DB), всё это в виде единого prometheus приложения.

И тот либо софт умеет выдавать данные в формате для prometheus — тут всё хорошо;
а если не умеет (redis, он вообще не знает про http) — тут есть redis-exporter,
который собирает данные из redis, а сам предоставляет http. Но это уже второй
контейнер — redis exporter.

TSDB поддерживает:

- time based retention (учитывает время сколько хранить метрики)
- 1-2 bytes per sample
- хорошо переносит series churn (учитывает большие объемы данных)
- только локально: никакой кластеризации и репликации
- ultra IOpS-effective

У нас есть target-ы, которые надо мониторинг, она берется из API kubernetes,
prometheus умеет искользовать этот API, получая оттуда target-ы.
Что у нас в таблице:

- target в виде URL
- время опроса
- label = namespace/service/pod

У прометея есть YAML конфиг, где можно опрелять job-ы (job_name), там описывается:

- как подключиться,
- какие объекты получать,
- как отфильтровывать,
- какие лейблы вытаскивать.

У kubernetes есть master, который необходимо мониторить, он привязан всегда к ноде;
kubelet стартует мастера и докер (создает докер контейнер) — это некая прослойка между ними,
  с него можно собирать все метрики по контейнерам, и там уже указаны под, неймспейс;
  есть также node-exporter — базовые системные метрики.

Компоненты kubernetes, которые надо мониторить:

- DNS
- ingress nginx
- soft

Grafana забирает данные из TSDB — PromQL.
Также в prometheus есть evaluator, он тоже умеет PromQL
и отправляет алерты с использованием alertmanager (внутри JSON).

У prometheus есть GET /federate, по которой отдаёт все метрики.
И есть longterm prometheus, который собирает метрики с других prometheus;
там данные хранятся дольше, но с 5-минутной детализацией.


# Тонкая настройка балансировки нагрузки

[envoy](https://www.envoyproxy.io/)

Бизнес волнуют ошибки и стабильность работы (те же 500-ки),
такие пики есть почти у всех и не все на них реагируют,
и не докапываются над причиной.

Почему этим надо заниматься:

- много релизов ежедневно
- kubernetes rollout с т.з. балансировки — как в новую школу
- микросервисы: теперь все общаются по сети

Nginx из коробки не делает ретрай при плохом ответе,
нужны повторные попытки — хотим любой ценой ответить
пользователю, но и не стоит перегружать сервера.

Неудачная попытка:

- nginx errors + timeouts (proxy_send(read)_timeout, есть обработка статусов)
- haproxy timeout connect
- envoy connect failure + refused-stream (есть обработка timeout-ов,
  timeout || per_try_timeout и статус-кодов)

conn timeout: net.ipv4.tcp_abort_on_overflow + маленький backlog — **fail fast**.
req timeout: есть описание выше для nginx и envoy;
  per_try_timeout = k * max(k > 0.5) — оптимистичные 2 попытки
  гарантированные 2 попытки: k = 0.5
Выбор timeout-а — поиск краевых случаев и учет большого количества критериев.
«Если кобыла сдохла, слезь.»

Спекулятивные ретраи: вероятно запрос не уложится timeout, делаем ретрай,
  но не прерываем первый запрос — и берем ответ от приложения, ответившего быстрее.
  speculative_retry = N ms | Mth percentile

Точка невозврата (V1 — из авиации, когда тормозить уже нельзя и обязаны взлететь):

- если начали передавать ответ, ничего исправить нельзя (делаем graceful shutown)
- если умный клиент, то ретрай на клиент-сайде (например приложение)

Если у нас есть долгий запрос, например в базу, и он гарантированно не укладывается в timeout,
то таким образом можно убить базу, поэтому ретраи нужно ограничивать:

- nginx: proxy_next_upstream_tries (и бонусом fail_timeout)
- envoy: num_retries

Как сделать так, чтобы не попадать на отключенные серверы — health-check,
но не стоит пренебрегать ретраями и их кол-вом попыток.
Healthchecks:

- nginx только в plus
- envoy, есть panic mode, если какой-то % хостов забанен, то healthcheck врет и все хосты живы

Envoy — разрабатывался в lyft, написан на c++:

- между попытками envoy делает паузы
- случайное значение в возрастающем интервале
- первый трерая через 0-24ms
- второй 0-74ms

Circut breaking:
когда бекенд тупит, мы его добиваем (пользователи начинают нажимать F5),
в этом ситуации стоит запросы не посылать — для этого и есть circut breaking,
3 состояния:

- closed
- open (посылаем маленькую часть запросов и есть всё хорошо — closed, всё плохо — open)
- half-open

ok.meter меняли в nginx limit req руками при одной из первых проблем;
а дальше уже была придумана логика отстрела запросов.

Epsilon-greedy:
по принципу выбора лучших на оснований первых запросов
и постоянного наливания на серверы какой-то небольшой доли,
а основной потом льем на быстрые серверы.
Outliner detection envoy:

- outliner баним на N ms
- с ростом количества банов N растет
- max_ejection_percent


# Распознавание лиц

Для каждого пользовательского фото свой вектор.
Обратное распространение ошибки — один из алгоритмов построения функции.

Сверточные нейросети:
есть некоторое ядро, которым мы сканируем наше изображение и заполняем
матрицу откликами (наибольшим числом совпадающих элементов), дальше
сворачиваем размерность, откуда выбираем наибольшее значение элементов:
4x4 -сворачивается-> 2x2

MTCNN — точность 90%, можно использовать CPU,
R-FCNN — точность выше, работает на GPU (дорогая функция).

MTCNN: определяется лицо и его точность, затем картинка резайзится (коэф. 0.7),
и снова вычисляется вектор, чтобы учитывать разные размеры.
Для фото оставляем лишь те области, которые дают максимальный отклик — грубый фильтр — первичный поиск лиц,
при этом blob detector занимает какое-то время.
На выходе первого этапа есть грубая оценка наличия лица на фото,
на втором этаме больее точное очертание лица и scale factor не 0.7, а 0.5.
Худший случай — стали укладываться на 250ms.

Векторизатор.
У нас есть два вектора, мы вводим на низ косинусную меру.
Сопоставляем каждому лицу «щи» (вектор на сфере).
FaceNet, DeepFace, DeepID3 — примеры Face vectorized.

Есть точность и полнота:
точность — взяли все положительные случаи и поделили на все, оценили погрешность
полнота — основывается но большой количестве входных данных

Функция потерь (softmax, tripetloss) — должна заставить работать нейросеть так,
как в дальнейшем мы будем использовать в production. Подбирается своя loss function.
loss(x,t) = max(Mp - cos(x, Ct), 0)^2 + sum(max(cos(x,Ci) - Mn, 0)^2)
после этого точность возросла (но незначительно) и выросла полнота.

Правда свежая версия FaceNet дает ощутимо меньшую погрешность, но пока ОК впереди по точности.
Также расширить данные нейросети можно с помощью поворота фото, фотошопа шапок и пр.

MTCNN страдает falce positive (~2%), но все классы (типы объектов) могут быть
также соотнесены классам, поэтому мультфильмы и статуи ОК не стали распознавать, как и детей.

Поиск по лицам: проблема поиска векторов — либо надо много железа, либо теряем полноту, либо точность.
Используют тут GPU, 2m пользователей по поиску на карту.
ОК тут немного упростили задачу и взяли друзей дрйщей и колиество искомых векторов снижено.

Результат:

- detection: 140ms, 1500/s
- vectorized: 20ms, 2000/s
- search: 8ms, 180M/s

Кластеризация лиц по фото:

- affinity propagation
- mean-shift
- dbscan (остановились на нем)
- birch

Затем анализ аватара и сопоставление ему кластера, поиск
происходит на сновании веса кластера по фото. Если не выделяется
фаворит, то выбирается ещё некоторое количество фото.
Дальше, если пользователь отмечается, что на фото — не он, стирается вектор.


# Как устроены корутины?

История начинается с подпрограмм, имеет точку входа и выхода:
A()  B() C()
 |   |
 |        |
 |   |
 |
 |

Ещё есть треды.
Корутны — когда мы сами решаем, когда переключаться между тредами.
Зачем они нужны:

- скорость:
  - нет блокировок, не нужны примитивы синхронизации
  - не инициируется context switching
- элегантность

Корутина — по сути использование goto.
Мы живем в мире вытесняющей многозадачности,
а корутина — сущность из мира кооперативной многозадачности.

Любой уровень абстракции — мы что-то теряем, в чем overhead корутины:

- стек, когда мы в корутине, мы можем выйти в середине функции,
  мы должны сохранять стек при выходе и восстановить его при возврате;
  ну или не использовать стек
- планировщик (мы управляем процессом сами), создатели марсохода pathfinder
  накосячили в планировщике и чуть его не потеряли

Корутины работают в рамках одного ядра; когда много ядер:

- забить
- использовать golang?

libcoro: coro.h
Есть несколько бекендов, в зависимости от бекенда разные корутины.

- CORO_SJLJ (long jump, типа goto между функциями),
  int setjmp(jmp_buf env); — куда вернёмся
  void longjump (jmp_buf env, int value);
- CORO_UCONTEXT, особых различий нет, тоже позволяет сохранить и восстановить контекст
- CORO_PTHREAD, как сделать корутины с тредов:
  в контексте есть thread id, есть один глобальный mutex на всю программу;
  мы шлем сигнал соседнему потоку, а этот поток засыпает;
- CORO_ASM — быстро работает
- CORO_FIBER. в win есть fiber, где есть уже готовая абстракция

Как выбрать бекенд:
- если не win, то когда возможно CORO_ASM
longjmp — часть стандарта языка
ucontext — часть POSIX, но deprecated
...
- CORO_PTHREAD

libtask:

- планировщик их коробки
- есть функции для работы с файлами и сокетами
- только ucontext

Можно ли без стека? — Есть protothreads.
Мы передайем в функцию параметр с какой строки её выполнять.


# Автоматический, надежный и управляемый деплой

Что такое хороший деплой? — Правильные ответы на вопросы:

- сколько нужно человекочасов, чтобы доставить код в prod
- как насчет роллебков, gree/blue/canary/gradual
- насколько прозрачен процесс и какие следы он оставялет

По большей части речь пойдет про chef.
Что получилось на выходе:

- 10 команд
- 40 деплоев в день
- ~1.5 kSLoC (полторы тысячи строк)
- 30min на выкладку кода в prod (быстрее и чаще можно, но не нужно)

Раньше было: разработчик передает фичи админам и дальше деплой,
какими-то кастомными скриптами. Дальше — использование менеджеров конфигураций.
Дальше cookbooks, roles, env разрослись и превратилось в огромную «кучу».

- тут ввели версионирование артефактов
- перенесли сборку кукбуков в CI-систему: проверка синтаксиса
- настройки приложений в версионированные роли
- фиксация версии кукбуков

DEV/OPS -> Repo -> CI --packages roles--> storage -> <MAGIC> -<> staging/production

Задачи:

- автоматически
- надежно
- комплаенс: перекрестное ревью и пр. change management
- оставление логов и др. следов

Таск-трекер для деплоя?

- деплой — тоже задача
- есть уже таск-трекер
- люди умеют им пользоваться
- есть API и UI

Связка CI --pakcages roles--> storage остается, а дальше есть Deloy task:

- куда попадают нотификации из CI и которые осуществляет деплой,
- на вход от разработчика «необходим деплой»,
- получение всей информации VCAD (разные версии/артефакты).

Миграция:

- 80% всех усилий
- самое главное в автоматизации — различать manual work и hand job
- заинтересуйте людей — митапы, презентации
- простота и документация необходимы, правда люди всё-равно не будут читать

Управление деплоем:

- одна активная таска (если две открытых таски, то может образоваться замкнутый круг задач)
- иерархия тасков
- обратная связь от системы деплоя (то, что код задеплоен, не значит, что он работает)
- ChatOps — гораздо приятнее таск-трекера, свой skype-бот

1. Хранение стейта в таск-трекере, не храните его в системе деплоя
2. Делайте ретраи, если работаете с каким-либо API, и не блокируйтесь на его ошибках
3. Разделяйте временные и перманентные ошибки
4. Считайте checksum артефактов (иногда бились артефакты при скачивании или был заменен на сторадже)

Чаты (создавались на любое действие, где нужно несколько людей, в том числе и на деплой):

- это работает, но ужасно отвлекает тех, кто работает над этими чатами
- придумали on-call > bau (business as usual): взяли инженера и он был входной точкой для всех остальных,
  он ужасно страдает, но вся остальная команда работает
- важна культура общения в чатах (любого человека легко идентифицировать по логину, выделение дежурного)
- в чат можно добавить любого человека и сразу описать что он него хотят

TL;DR

- деплой — не только дело техники, но и людей
- люди любят простоту и прозрачность
- управление деплоем хорошо ложится на task tracker
- программировать легко, но самое сложное — то, что нужно эту фичу продать/поддерживать


# Сетевые базисы kubernetes

[kubernetes-comic](cloud.google.com/kubernetes-engine/kubernetes-comic)

Kubernetes сейчас:

- второй по популярности проект после linux
- у него появилась сертификация
- по последнему kubicom: >54% компаний его используют

Основной плюс kubernetes — очень легко начать.

Базовый дизайн: у каждого пода есть IP-адрес, на veth, как правило.
Но в поде может быть несколько контейнеров.

Для чего можно использовать поды с несколькими контейнерами?

Kubernetes запускает специальный контейнер на паузу (paused),
как только мы создаем такой первый контейнер — мы выделяем сетевой интерфейс,
а дальше мы можем разделять сетевой интерфейс (можем использовать общий сетевой
стек в рамках одного POD).
Paused контейнеры лишь сетевая оснастка для работы сетевого стека в рамках пода.

Kubernetes следит за использованием IP-адресов в сети подов, то есть
обеспечивает связность таким образом, чтобы не дублировать IP-адреса подов.

Сеть подов:

- имя моста cbr0 (custom bridge)
- сеть подов используется при логировании и отладке
- gcloud describe cluster <name> | grep clusterIpv4Cidr # можно узнать используемое пространство имен
- нужна явная маршрутизация для 10.0.0.0/8 (нет NAT, чтобы обеспечить правильный роутинг)

Сеть вервисов:

- под эфемерен (сейчас под есть, а завтра может не быть)
- IP-адреса подов будут меняться

Как не заниматься менеджментом адресов подов — поставить балансировщик.
Требования к балансировщику:

- отказоустойчивый
- должен знать о серверах или объектах
- должен понимать статус объектов

Service объединяет за собой поды и снаружи у него новый IP-адрес.
kube-proxy: этот IP-аддрес сервиса не фигурирует на интерфейсах,
  k-p он у себя запоминает соответствие адресов и через netfilter
  сообщает о трасляции адресов.
Эта конструкция гарантирует доступность сервиса.
K-p должен запускаться как daemon set (issue #23225 open).

Один минус: теряется source IP для запросов вне кластера.
Как запускать трафика снаружи кластера?
Можно попытаться обратиться к адресу, который выдал сервису, когда он создавался
(ну и поправить маршрут о том, как роутить пакет); netfilter на хосте сделает подмену,
и всё будет работать пока работает хост.
Но хосты тоже эфемерны. А роутер не следит за статусом сервера.

Собираем балансировщик и направляем на него трафик?
Сам балансировщик должен находиться с сети сервисов, чтобы можно было оценивать состояние.
Эта конструкция не будет работать без node port, который позволяет
запустить на каждом хосте прослушивание для какого-то порта и в качестве
DST выбирается адрес хоста.
Node port используется тот же kube-proxy, то есть получается у нас используется дважды
node port — первый для порта на стороне хоста, второй, для трансляции адреса на внутренний.
Есть недостатки:

- портов ограниченное количество (ограничение на 2768 портов)
- порты можно явно создавать

Функционал load balancer соответствует node port; но load balancer выделяет:

+ дает внешний IP/port
- создание занимает минуты
- нет терминации TLS
- нет virtualhosts и pathrouting

В контраст load balancer есть ingress — который умеет терминацию TLS,
поддерживает виртуальные хосты и маршруты по path.

Выводы:

- сетевая система спроектирована «элегантно»
- идет активное развитие: ingress, eBPF, flannel
- оркестратор взрослеет: service mesh
  см. «Микровервисы и Istio», moscowpython
- сетевые проблемы в production не просто исследовать
  см. «Magic of k8s netwroking», kubecon18


# QUIC, TLS 1.3, DNS-over-HTTPS

Cloudflare, при включении HTTP/2: увидели 0% входящих запросов по HTTP/2.
Если между браузером и серверов стоит облако, то облако само терминирует SSL;
то есть выступает в качестве reverse proxy. Все облака умеют HTTP/2 на
стороне браузера, но не умеют ходить в upstream по HTTP/2.

**QUIC**

- новый протокол для транспорта
- основан на UDP
- первый черновик 06.2016
- текущая версия 12
- не является стандартом

[vs TCP](https://blog.apnic.net/2018/01/29/measuring-quic-vs-tcp-mobile-desktop/):
во всех типичных условиях QUIC работает лучше TCP, но реализация QUIC существенно
хуже, если сеть не гарантирует порядок доставки пакетов.
QUIC реализовать в userspace и на мобильных устройствах проводит 58% в состоянии
Application limited, например, когда отправили данные и ждем подтверждение.
То есть для мобильных устройств и медленной сети QUIC хуже, его дизайн — эксперимент.

Предложено было выделить QUIC как обычную траспортную функцию, а дальше QUIC-over-DTLS:

- DTLS: TLS-over-UDP
- QUIC-over-TLD-over-UDP
- stream 0 (отвечает за установление соединения и шифрование),
  если у нас есть потеря потеря, то реализация может случайно шифровать пакеты

Почему так? QUIC — agile-подход:

- черновик
- обсуждение (feedback собирается со всех сторон)
- гугл внедряет изменение в chrome
- N+=1, goto 1

В QUIC альтернатива TCP-handshake переехала в userspace, вместо того,
что раньше tcp handshake реализовывался на системной уровне.
Почему? Участники хотят видеть протокол внедренным, они не хотят
ожидать, пока все ОС обновятся. Да, это дороже с точки зрения ресурсов
но для Google это не проблема:
«to deploy QIOC in any maching even without IS support»

QUIC в linux kernel? Работа над этим не ведется.

Отправка UDP датаграмм в Linux существенно дороже:

- дороже lookup по таблице маршрутизации
- large segment offload

[pdf](http://vger.kernel.org/netconf2017_files/rx_hardening_and_udp_gso.pdf)

Первое, что нужно сделать перед выкладкой в prod: провести **benchmark**, как примеры выше.

**IPv6**

- happy eyeballs (подход: было много проблема с жалобами IPv6 со стотоны пользователей,
  IPv6 работает где-то дома, а подключившись к другой сети не работает, поэтому если не
  нашли в течении какого-то времени не нашли IPv6, то fallback на IPv4):
  мистические проблемы с реализацией (очень долгое переключение)
- malicious activity (локальная сеть IPv6 = /64):
  blacklist и проблемы с памятью
Результат: проблемы с внедрением.

**DNSSEC**
[Тренд роста](https://blog.apnic.net/wp-content/uploads/2018/02/fig1.png)
https://blog.apnic.net/2018/02/26/peak-dnssec/

Его развертывание остановилось и пошел обратный процесс.
Разговоров про DNS очень много, очень много различных предложений реализация —
DNS-over-whatever, как результат:
[How many features can we add to this protocol before it  breaks?](https://datatracker.ietf.org/meeting/101/materials/slides-101-dnsop-sessa-the-dns-camel-01)

**TLS v1.3**
TLS1.0 — 3 года разработки, TLS1.1 — 7 лет, TLS1.3 — 10лет (27 черновик).
Очень много проблем при реализации.

Выводы:

- внедрение нового протокола — это расписанный план внедрения,
  с оценкой целесообразности и benchmark, а не «немного что-то поменять»
- нужно участвовать в процессе
- участвовать в разработке протокола


# Обобщенные табличные выражения и оконные функции MySQL 8.0

В 8.0 добавились:

- улучшение в unicode
- улучшение в JSON (in-palce обновления)
- оптимизация innodb
- улучшение репликации
- новый словарь данных и атомарный DDL
- роли и улучшение в системе привилегий
- CTE и оконные функции

CTE:
__WITH cte AS__ (subquery) SELECT ... FROM cte, r1
Подзапросы также могут быть CTE.
Чем лучше производных таблиц:

- лучше читается
- проще выстраивать цепочки (производные таблицы не могут ссылаться на соседей => вложенность),
  в CTE можем ссылаться на предыдущие словарные выражения
- на проивзодные таблицы нельзя ссылаться несколько раз в запросе (есть дублирование производных таблиц)
- быстрее работают

Стратегии вычисления табличных выражений:

1. Материализация
- выполняем select
- сохраняем во временной таблицы
- выполняем второй select
- в отличие от производных таблиц оптимизатор может создать индекс во временной таблицы
2. Слияние

Материализация может быть лучше слияния, когда внешние условия не селективны или нет индексов.
Как выбирать стратегию:

- эвристики
- технические ограничения на поддержку слияния (derived_merge)

Про рекурсивные выражения — см. slide: __WITH RECURSIVE cte AS__
Рекурсивный подзапрос может ссылаться на табличное выражение:

- только во FROM части и один раз
- не в подзапросе
- не в качестве правой таблицы left join

и не может содержать:

- group by
- order by, limit и distinct

Вычисление происхдит так, как если бы на каждом этапе обрабатывалось по одной строке.

До версии 8.0 — использовали аггрегатные функции, но мы теряли часть информации,
в оконных функциях не теряем индивидуальные данные.
Оконные функции:

- функция + over
- большинство функций могут быть как оконные: COUNT, etc
- секционирование с помощью PARTITION BY <expr>
- упорядочивание внутри секции ORDER BY
- задание рамок окна (по умолчанию вся секция):
  ROWS n PRECEDING/FOLLOWING
  RANGE — когда задаем диапазон значений для подходящих строк (требует ORDER BY)

Окна можно также именовать: __WINDOW w AS__.
Выполняется оконная функция после GROUP BY/HAVING, но до ORDER BY/LIMIT/DISTINCT.
Для вычисления оконной нужна временная таблица и буфер окна, чтобы хранить текущее значение окна.
Буфер окна не нужен в случае функций RANK, DENSE_RANK, ROW_NUMBER, etc
  или для агрегатных функций с окном с растущей верней границей.
Непотоковые функции CUME_DIST, SUM () OVER () требуют буффера.

Рекомендации по потоковым функциям:

- используйте именованные окна — проще читать
- потоковые функции быстрее
- MAX/MIN не поддерживают инверсию


# Конвейер поставки виртуальных машин

Jetbrains

- 15000 билдов в день
- 500 build agent-ов (большинство — VM)
- для сборки свой teamcity, а также ещё один в AWS
- надо постоянно обновлять тестовое окружение

Docker?

+ управление на уровне билда
+ герметичные билды
+ локальная повторяемость
- windows
- mac os
- 9000 конфигурация

Почему vSphere?

- билды требовательны к CPU и IO (в AWS дороже)
- лицензии на windows
- в 6 раз дешевле, чем AWS (700k$ в год)
- mac pro

AWS:

- производительность разработчиков выше
- spot instances — на 60% дешевле
- эластичность — больше машин перед релизом
- цены падают
- autoscaling: нагрузка меняется в течении дня

Конвейер CD:

- version control (git)
- build: новая VM, настойка софта и снятие образа с диска
- test: запускаем инстанс и выполняем тесты на правильность её окружения, удаляем инстанс
- release: помечают сборку как стабильную
- deploy

Build+test+release не влияют на production.

Immutable infrastructure:

- gо конвейеру едут images, а не скрипты;
- все инстансы байт-в-байт одинаковые;
- новый инстанс стартует быстро;
- есть небольшой downtime: удаляют текущие и создаем новые инстансы
- трутся кеши.

Hashicorp packer-ом создают новую машину, но с vSphere он работает достаточно плохо,
в результате [packer-builder-vshepere](https://github.com/jetbrains-infra/packer-builder-vsphere)
Внутри машин скрипты на bash/powershell, таких скриптов порядка 200 штук плоским списком в репозитории —
самое главное достоинство такого решения — простота.

Почему не chef/ansible:

- знакомый синтаксис
- нет потребностей в фичах (local mode, нет апгрейдов — идемпотентность не нужна)
- работает на windows

Тесты:

- проверяют артефакты, не скрипты
- простые
- падают редко и есть не везде
- помогают контрибьюторам

Используют packer-плагин для teamcity.


# Профессиональное выгорание: кто виноват и что делать

Фазы по временной шкале:

- предупреждающая фаза: чрезмерное участие -> истощение
- деперсонализация: перекладывание вины / безразличие / деньги
- эмоциональные реакции: депрессия / агрессия
- деструктивное поведение: ригидность / отказ от хобби
- психосоматика: бессонница / иммунитет--
- бессмысленности жизни

Симптоматика проф. выгорания:

1. шопоголизм
2. заедание проблем
3. алкоголь
4. промискуитет
5. наркотики
6. экстремальный сорт
7. игромания
8. уход в работу (!)

Усталость — то, что проходит после отпуска, а астения — нет,
значит уже наступила некоторая степень выгорания.

5 гормонов счастья:

- дофамин
- адреналин (волнующие активности)
- серотонин (прогулки, сон, свежий воздух)
- эндорфин (секс, силовые упражнения)
- окситоцин

Разные активности приводят в выработке того или иного гормона,
когда мы уходим в работу — начинает много вырабатываться дофамина;
затем снижается чувствительность рецепторов к дофамину,
происходит эффект привыкания.

Sabbatical — длительный творческий отпуск.
После отдыха ответ «чем заниматься дальше?» не пришел :)
Один из факторов выгорания — размытость личных целей.

4 направления поиска себя:

- вспомнить детские мечты
- история силы: вспомнить моменты в жизни, когда было много-много энергии (прошел олимпиаду, например)
- работа с тенью: борьба и понимание того, что не устраивает
- зов: какие-то знаки — ловить их

Понимание своей цели и миссии недостаточно, то, что довело до выгорания — часть нас.
Оказалась проблема в образе мыслей: иногда шел наперекор своим идеям во благо идей
другого партнера, а негатив накапливается.

«Будьте внимательны к мыслям. Они — начало поступков» © Лао Цзы

В голове очень тяжело разобраться в одиночку:
мысли / поведение / эмоции / телесные ощущение — всё взаимосвязано
В итоге после Sabbatical энергии не появилось, а после психотерапевта — очень.

**Советы**

1. Уберите дофаминовые стимуляторы (сериалы, кофе, соц. сети, игры)
2. Сядьте на низкоинформационную диету (выработка картизола)
3. Не читать почту по утрам (если что-то нужно, люди позвонят)
4. Выходные + внезапные выходные + отпуск
5. Разбирайтесь и понимайте как работает ваша голова
6. Не стесняйтесь обратиться к специалисту
7. Думайте, чего вы действительно хотите

Главные навыки управленца:

- соблюдать баланс (не увлекаться чрезмерно)
- находить смысл, что делаешь
- осознанность (понимание того, что с тобой происходит)

Год отпуска не нужен.
