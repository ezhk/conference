# Эксплуатация contaier-based-инфраструктур
До микросервисов — всё на отдельном железе, проблемы:
- сложно разрабатывать
- сложно управлять зависимостями

Микросервисы:
- траблшутить стало проще
- нет зависимостей
- простой сервис вместо монолита

В коде были вызовы функций сторонних модулей, стали сетевые вызовы.
Этапы развития:
1. infrastructure as a code — завиксировать все взаимодействия
2. docker
3. kubernetes

Docker:
- было сложно собирать deb/rpm
- уложнили сеть, чтобы не было коллищий
- внутри: namespaces, cgroups + tooling
Контейнер легче VS, ниже overhead по CPU, например.

Текущее настоящее:
- docker-, kube-  — на отдельном облаке железа
- БД также на отдельном железе
- resource-intersitive приложения также на отдельном железе
В результате абсолютной простоты так и не получилось до конца.

Какова финальная задача:
- распределение ресурсов между сервисами
- как избежать простаивания ресурсов
- понимание дополнительной ёмкости

Cgroups:
- **CPU**:
  - shares — пропорция выделения CPU time
  - quota — ограничение CPU времени в ед. реального времени
    если процесс потратил свое время, то ресурсы не выделяются — throttling<br />
    `docket --cpu-period --cpu-quota`
  - cpusets — привязка процессов к конкретным ядрам
Не забываем performance в scaling_governor (пример: один процесс отвечал медленнее до запуска второго процесса).

Стабильность с квотами
- знаем предел производительности
- можем посчитать утилизацию docker/<id>/cpiacct.usage
- если уперлись в quota — растет nr_throttled и throttled_time docker/<id>/cpu.stat

Как распределять ресурсы:
- делим сервер на слоты без overselling для latency-sensitive сервисов (quota)
- для повышенной нагрузки готовим ещё столько сервиса, чтобы потребление < N%

Но фоновые сервисы остаются: слотам выставляем максимальный --cpu-shares,<br />
чтобы фоновые задачи были менее приоритетнее — *не помогло*.<br />
CFS:
- достаточно дискретен, CPU time выделяется слотами
- sysctl -a|grep kernel.sched_
- изменение интервала квоты с 1ms > 50ms — *помогло*.

- **Memory**:
  - <cgroup>/memory.stat
  - можем ограничить размер памяти группой
  - можем узнать сколько съел page_cache
  pcstat <filepath> — проверка наличия файла в кеше.<br />
  `docker --memory`

- **blkio**:
  - всё по аналогии CPU
  - лимиты по iops/traffic на r/w
  - можно настроить для конкретных дисков
  `docker --device-read-iops`<br />
  Метрики: `cgroup/blkio/<id>/blkio.throttle.io_services`<br />
  `docker --blkio-weight`

  IO sched:
  - cfq настроить не получилось
  - deadline лучше, когда важно latency > `echo deadline > queue/scheduler`<br />
  Результаты: работа приоритетов зависит от планировщика IO.
**«Perf очень крутой.»**


# Linux kernel TLS & HTTPS
HTTPS DDoS:
- ассиметричный DDoS (компрессия, TLS) — сервер тратит больше ресурсов, чем клиент
- множество IP-адресов — сложность фильтрации (используется machine learning)
- Машинное обучение — временной лаг при срабатывании
TCP stream filter:
- iptables strings (сканирование заголовков, URI, Cookie),
  но при пересечении границы пакета нужными данными это не работает
- web accelerator (он парсит заголовки, но не подходит для фильтрации)
- IPS/IDS (фильтрация заголовков от узлов),
    - не собирает трафик, не участвует в его приеме
    - также нужно терминировать SSL — двойная обработка запроса

Какая система нам нужна:
- терминация TLS
- должна быстро парсить HTTP
- должны быть встроены механизмы фильтрации
- быстрый web-cache: следствие того, что есть проседание по классификации машинного обучения,<br />
  поэтому на первое время надо успевать обрабатывать запросы; также могут быть и будут ложные срабатывания.

Web-accelerator:
- хорошо отрабатывает контент
- умеют хорошо rewrite/ACL
- делают фильтрацию
но все они проектировались давно, фильтрацию<br />
делают не очень хорошо (http flood их положит).

Kernel-mode web-acc: TUX, kHTTPd
- те же сокеты и треды
- zero-copy > sendfile() (тут от них отказались)

Почему работают мелденно web-accelerators:
- медленный интерфейс сетевого IO
- есть копирование системных вызовов
- иногда блокировать дороже, чем отдать из кеша
- используют libc вызовы или медленные FSM

Fast network IO: kernel bypass подходы
- netmap (10Gb per core)/DPDK/PR_RING,<br />
  быстро, но на очень простых приложениях,<br />
  пока не нужно парсить заголовки
- TCP/IP стеку нужны механизмы синхронизации и быстрая аллокация памяти,<br />
  в user-space есть read-copy update, но он медленный;<br />
  spinlock сделал в libc плохо, в ядре более прогрессивный MCS (учитывает конкуренцию между ядрами);<br />
  нужно реализовать buddy-allocator

Kernel SSL/TLS, история:
- KSSL, портированный openssl в ядро linux
- solaris ssl proxy, но в основном для старых CPU-архитектур

Ядро 4.13: facebook/redhat добавили поддержку,<br />
он может оффлоадить только стрим (только симметричное шифрование),<br />
**не умеет handshake**.<br />
CONFIG_TLS нужно дополнительно включать.<br />
Mellanox использует этот же интерфейс kTLS.<br />
Что сейчас плохо сделано:
- facebook решал свою задачу с установленными сессиями,<br />
  их не очень интересовали handshake новых пользователей, а для нас они важны<br />
- TLS 1.3 умеет быстро устанавливать handshake (1-RTT handshake), но по-прежнему есть TLS 1.2
- TLS renegotiation (клиент всегда может попросить это сделать)

Сейчас в процессе:
- встроенный http парсер
- HTTPS работает также как и TCP/IP

Большой недостатоток user-space HTTP proxy:<br />
множественное копирование данных, в том числе и из-за переключения контекста ядра.<br />

Синхронные сокеты:<br />
HTTP pipeline здорово увеличивает производительность, но нужны локи при доступе в TCB.<br />
Но ходить к локам с разных ядер долго.<br />
Быстрый трансфер inter-CPU — используется ring-buffer.<br />

skb page allocator, zero-copy HTTP msg:<br />
например, нужно изменить заголовки пакеты;<br />
мы разделяем контент на 2 и посередине правим заголовок<br />
и склеиваем 3 блока — это быстрее копирования skb.<br />

В процессе работы также:
- TLS 1.3 puzzles > serverhello:
  - cookie (simple echo)
  - sha 256/512
- sticky cookie
- zero-copy space transfer

[Tempesta](https://github.com/tempesta-tech/tempesta)


# High speed load balancing from the linux kernel
[Zevenet](www.zevenet.com)

Network layer filtering:
- raw prerouting
- mangle prerouting > marking
- nat prerouting > rewrite destination
- routing
- nat postrouting > rewrite source

Using *nftables* for different expressions:<br />
  nth, random, hashing, etc.<br />
  Change dst mac-addresses in this case.<br />
LVS-NAT/DSR slower than NFT-NAT/DSR.<br />

*zvnftd* using nft, make healt check possible.<br />
[github](https://github.comf/zevenet)


# Dataplane networking acceleration woth OpenDataPlane
[ODP](https://github.com/Linaro/odp)

Почему dataplane уходит в user-stack и не делается в ядре linux:
- задержки (возможно процессор уже занят обработкой прерывания или когда сработает softirq)
- проблема копирования пакетов из области ядра в приложение
- сложная манипуляция с skbuf
- если крешиться компонента ядра — это хуже креша приложения

Dataplane types:
- hardware
- software
- software defined — приложение определяет функциональность,
  и если оборудование может сделать это в железе, то оно
  делается там

OpenDataPlane — описывает API, как управлять железом, чтобы писать кроссплатформенные приложения.<br />
Похожие проекты, но ориентированные больше на работу с сетевыми картами:
- PFring
- Netmap
- DPDK

Bottleneck — PCI-шина, express 5.0 имеет пропускную способность 60Gb/sec.<br />

ODP components:<br />
=RX=> PKTIO (packet IO) => classificator => набор очередей и их процессинг (odp_thread) => traffic manager => PKTIO (возвращение их в линию) =TX=><br />
Модель ODP:
- основной процесс выделяет память и настраивает компоненты
- воркеры, процессят трафик на определенном ядре

ODP внутри:<br />
odp_pktio_t — абстрактный тип, достаточно указать его имя<br />
Тип так выбран потому, что у разных вендоров по-разному обозначаются<br />
устройства ввода-вывода. Похож на абстракцию skbuf в Linux.<br />

Базовый компонент ODP — очереди, куда пакет попадает в соответствии со своим flow.<br />
Очередь software, если оборудование не поддерживает аппаратную очередь.<br />
Они, в свою очередь, могут быть параллельные, или ordered — с сохранением порядка.<br />

Режимы работы:
- без участия sched, когда берем пакет и перемещаем его в другую очередь
- odp_schedule() — также может быть реализован аппаратно

Классификатор — блок, который стоит на входе;<br />
внутрь можно загружать правила, как пакеты будут распределяться на очереди.<br />
На выходе стоит traffic manager, который делает QoS, например, wred.<br />

OpenFastPath.org — IP stack on ODP<br />
https://github.com/OpenFastPath/nginx_ofp


# Настройка kubernetes: tips & tricks
Как работает kubernetes: для разработчика — есть код, kubectl и облако.<br />
На самом деле:
- control plain — набор процессов для обеспечения работы кластера
- worker node — ноды, где работают приложения

*Control plane*
- apiserver — все компоненты через него, это stateless процесс, его можно спокойно scale-ить;<br />
  он открывает REST API, можно подписываться на обновления по фильтру — что и делают все компоненты.
- etcd — основное хранилище кластера не только текущего, но и желаемого.<br />
  Эту компоненту надо стазу расширять.
- sheduler — он просто прописывает соответствующую ноду в соответствии со спецификацией.
- controller manager — отвечает, чтобы желаемое состояние сервиса соответствовало действительно.
- dns
- сетевой addon (calico)

*Worker node*
- kube-proxy — подписывается на одновления apiserver-а, мониторит изменения,<br />
  конфигурирует сеть (правила iptables)
- kubelet — работает непосредственно с движком контейнера (create/delete container)

kubectl работает также с apiserver.

За чем следим:
- ноды: CPU, net, RAM
- инфраструктура: apiserver, etcd, scheduler

Проблемы:
- «нехватка» ресурсов — стало больше сущностей, за счет простоты разворачивания<br />
  разработчики активнее создавали dev-окружения: kube-scheduler.<br />
  *Нужно предвалидировать и устанавливать requests в зависимости от окружения*<br />
  Использовать LimitRange — наиболее правильный (объект в котором можно<br />
  задать число request-ов по умолчанию), выделять и ограничивать namespaces,<br />
  написать свой linter.
- высокий и «рваный» latency, обратить внимание на performance в scaling_governor<br />
- сетевые потери: под удар попадает dns (внутри resolv.conf добавляется 3 домена,<br />
  поэтому каждый запрос получим 6 обращений — nsdomain и A запросы)
- деградация, когда свыше 100 pod на хосте:

      $ kubectl get pods
      ...
      status
      container creating

  в логах при этом operation timeout: context deadline exceeded.<br />
  Приняли решение разбивать хосты: запуск vms как k8s worker nodes.<br />
  Запустили kvm-машины (для запуска множества конейнеров О_о).<br />

Мониторинг:
- kube-apiserver:
  - rate запросов
  - процентили
  - rate ошибок по запросам
- kube-dns:
  - dnsmasq: кеш перед kube-dns

        $ ps axuww
        ...
        dnsmasq --cache-size 5000

- kubelet:
  - rate запросов и ошибок

*Prometheus* умеет мониторить все компоненты, как worker node, так и control plane;<br />
он умеет ходить в /metrics — а все компоненты умеют корректно отвечать на этот запрос.<br />

kubernets — стоит воспринимать как ОС для кластера.<br />
При работе с kvm стоит использовать virtio в качестве сетевой подсистемы.


# Блокчейн. Lego для интересующихся
Классические банки — большие очереди, частично решают проблемы интернет-банки.<br />
Но хочется больше прозрачности, понятности, чтобы просто взять и одолжить людям деньги.<br />
Наш банк — это точка входа, но хорошо бы наши действия где-то запаковать —<br />
объединять в транзакции, поскольку их много. Также мы хотим проверять транзакции большими пачками.<br />
block (transactions (action)) — то есть блок включает транзакции, а те — абстракции.<br />

В итоге мы начинаем по сети передавать не деньги, а транзакции, чтобы избежать<br />
ситуации откуда и куда пришли данные — transaction pool, у которого есть время<br />
жизни и размер. Сеть должна быстро договориться с теми, кто крутит public node.<br />
Если транзакция не укладывается в лимиты — она выбрасывается.<br />

Дальше нодам надо договориться — consensus: pow, pos, dpos, в итоге система<br />
сформировала следующий блок на основе transaction pool и применяет его.<br />

Помимо этого для каждого блока вычисляется хэш с учетом предыдущих,<br />
в итоге у нас всегда есть зависимость:<br />

    {-; block0; hash1}
    {hash1; block1; hash2}

Также в блокчейнах есть индекса (в некоторых из них их надо предопределить),<br />
единственный способ его наполнить — просматривать на каждом блоке.<br />
Классические индексы — прямой и обратный — есть хеш от блока и его номер; также есть timestamp.<br />

Блокчейн — некоторый набор алгоритмов.<br />
https://github.com/kotbegemot


# Развитие БД в Dropbox
Все клиенты пользуются API, который взаимодействует с metaserver<br />
(хранится информация о файле), blockserver (где хранится о block-ах).<br />
Клиент обращается к metaserver за получением, там есть кеш,<br />
blockserver для записи, блоки хранятся где-то в block storage.<br />
Всё это на mysql.<br />

Metaserver изначально хранил информацию в Global DB, в дальнейшем шарды разъехались.<br />
Edgestore — графовый сторадж — абстрагирует mysql от клиентов и использует также cache,<br />
эту сущность использует metaserver.<br />
Blockserver расширился до Magic pocket.<br />

Базовая топология<br />

    DC1:                  DC2:
    master --> slave ---> pseudo master
           |-> slave          |-> slave

Magic pocket дублирует данные среди зон.<br />
Итогда pseudo master может быть master и тогда master-master репликация.<br />

Сингл инстанс (под mysql сразу выдяляется почти вся память = 310GB):
+ просто оперировать
+ простые фейловеры
- операции над один серверов
- шарды влияют друг на друга

Сейчас запускают мультиинстанс:
+ операции над инстансом
+ шарды почти не влияют друг на друга
+ микс разных категорий и классов
- сложнее управлять
- сложнее фейловеры

В процессе развития появился AFS — абстракция к zookeeper.

Любую базу можно восстановить за любой период времени на последние 6 дней.<br />
Проверяют бекапы, выборочно восстанавливают набор реплик.<br />
Используют xtrabackup.<br />
Бекапят binlog.<br />

Автоматизация:
- auto-replace (сервер потерялся и надо решить что произошло как можно быстрее)<br />
  Каждый сервис шлет heartbeat. В loop проверяем hearbeat, если достигнут threshold,<br />
  то с сервером проблемы; затем проверяется есть ли запись и какая-либо активность;<br />
  3 шагом проверяется не было ли изменений в топологии, вдруг сервер только добавлен,<br />
  сколько таких одновременных срабатываний произошло.<br />
- DBmanager — в нем есть логи, планировщики задач, точки синхронизации;<br />
  в нем есть клиент и есть планировщик для запуска операций;<br />
  на каждом хосте есть dbagent — куда передается то, что нужно сделать на сервере;
- naoru (небольшие задачи, типа замены дисков)/wheelhouse (для долгих процессов, типа обновления ядер на серврах)

Инциденты доставляются через PagerDuty.<br />
Каждый инцидент разбирается, их нужно отрабатывать.<br />
Принудительно тушат какую-то часть production серверов и проверяют<br />
сценарии работы. Приложения должны закладываться, что часть может отказать.


# Технологии хранения для больших проектов
Ceph — обеспечивает высокую достпуность данных, система обещала быть масштабируемой, гибкость.
  - Трехкратная репликация — очень дорого.
  - От EC пришлось отказаться.
  - Производительность записи 4-6GBps.<br />
  Предложенное решение — flash cache.

3 типа ФС в качестве больших кластерных систем:
- параллельные
- кластерные
- распределенные (к этому стремятся также первые два)

Общий набор функций таких ФС:
- прозрачность
- консистентность данных
- масштабируемость
- устойчивость к отказам
- балансировка нагрузки
- доступ к данным

Когда плох Shared Nothing (архитектура системы, когда нет общих ресурсов, когда общая только сеть):
- стоимость хранения критична (так как надо на 2-3 узла записать данные)
- высокая интенсивность записи
  - загружается сильно interconnect
- нагрузка
- требуемая производительность превышает производительность узла

Параллельные ФС:
+ обеспечивают доступ к одному набору данных от множества серверов
+ клиентское ПО эффективно параллелит нагрузку
+ большие размеры кластеров
+ зачастую — ассиметричная архитектура
В качестве примера Lustre.

**Lustre** обладает двумя основными политики stripe (файл разбивается на чанки):<br />
  RR (когда чанки случайным образом размазываются на ноды),<br />
  QoS (чанки распределяются по наименьшей заполненности, включается после RR).<br />
BeeGFS имеет API для приложений.<br />
Типичные ошибки Lustre:
- использование lustre в качестве основного хранение
  - оптимизация под высокую производительность
  - ребалансировка кластера

Короткие тезисы про другие ФС:<br />
**IBM GPFS** — привязка к оборудованию.<br />
**GfarmFS** — на множестве узлов ставятся доп. модули, которые помогают<br />
развернуть над локальными ФС некую глобальную сущность.


# Не так страшен терабит
Клиентом считают тех, кто размещает контент.

Egde (отдающий сервер):
- проверяет подписи на URL
- собирается статистика по стримам и клиентам
- 60k одновременных пользователей
- 74Gb/sec
FreeBSD 11 + zfs (l2arc + mirror) пул > zfs arc > nginx + lua<br />

Научились на старте отдавать 54Gb/sec (dual port chelsio T580-LP-CR), но CPU idle 28%.<br />
Но статистика не показывала как нагружен один PCI-express порт.

    В итоге несмотря на то, что сетевая карта PCI-e 16x в соотв. со спекой, на самом деле PCI-e 8x.
    Сюда добавить оверхед транспортного уровня
    (по PCI-e данные в виде блоков и размер блока по самому низкому устройству),
    и получили, что наибольший размер блока = 256байт — получили 56.4Gb/s.

Все 2-портовые карты — PCI-e 8x, и появился SFC9200 Solarflare:<br />
продукт сырой, который не совместим с оптическими модулями компании,<br />
в bios после изменения его настроек повторно попасть нельзя.<br />
Параллельно вышел Chelsio T62100-LP-CR — тоже несовместимость с оптическими модулями,<br />
и неравномерно прерываний на ядрах процессов (сделали меньшее кол-во очередей, а кол-во процессоров != степени 2).<br />
В итоге оба вендора исправили свои недостатки.<br />

Здесь сумели достичь 74Gb/s, тут CPU занят почти на 100%.<br />
Дальше либо модернизация софта, либо покупка новых процессоров.<br />

Столкнулись, что некоторые edge стали деградировать —<br />
перестали отдавать 40Gb/s трафика (до преодоления рубежа в 54):<br />
проблема оказалась в SSD Crucial.<br />
Эволюция закупки SSD (поскольку старые не производились): `MX100 > BX100 > MX200 > BX200 > MX300`.<br />
После окончания оперативной памяти идет запись на ZFS arc,<br />
увидели утилизацию SSD на запись, на 100%.<br />
Нашли бенчмарки, увидели, что начиная с MX200 сильная просадка по записи.<br />
Сейчас используют Samsung 850evo.<br />

SSL: теряют производительность на ~1% CPU load на 1Gb/s,<br />
при полной загрузке CPU (E5-2650v4) ~ 45Gb/sec.<br />

**P2P (WebRTC)**
- уменьшение стоимости
- используют технологии streamroot
- позволяет оффлоадить 70% трафика.

Как это работает:<br />
пользователь после получения playlist-а обращается к streamroot,<br />
который динамически формирует список пиров, где есть этот контент;<br />
дальше клиент устанавливает webrtc сессии с пирами;<br />
если контента нет, то в качестве fallback получит контент с SSD edge.<br />
Технология хорошо подходит как для стримов, так и video-on-demand;<br />
23 одновременных просмотра позволяют экономить 50% трафика.<br />

Качество обслуживания при этом улучшается.<br />

Технология работает на 80-90% пользовательских устройств,<br />
поддерживаются почти все декстопные браузеры; мобильные: chrome, safari iOS11.


# Пишем свой протокол поверх UDP или платформа потокового видео
Periscope — используется Wowza, стрим — RTMP, отдача — RTMP.<br />
Причины задержки в стриминге:
- сеть/протокол
  Протоколы:
  - потоковые: RTMP, webRTC, RTSP/RTP
  - сегментные: HLS, DASH, SMOOTH
- кодирование/декодирование: видео — набор кадров: i, p, b-frame
  - пробуем убрать b-frame;
  - уменьшаем длину ссылки до предыдущего кадрта — тюнинга кидека;
  - постоянный битрейт — включаем cdr.

RTMP — от adobe, основная проблема в использовании TCP:
- никто не застрахован от блокировки на начало очереди — при потере пакета;
- есть buffer: сеть стала работать хуже, битрейд мы понизим, но пакеты из<br />
  buffer-а никуда не делись и их надо отдать
- packet loss, а на беспроводных сетях 1-3% — норма

WebRTC — пренебрегает потерями (во всех непонятных ситуациях просто дропает),<br />
а также шифрует весь поток, что сказывается на производительности по CPU.<br />

RTP-стриминг — можно с помощью расширений построить UDP,<br />
но не все клиенты могут поддерживать расширения.<br />

MPEG-Dash: видео разбито на сегмента, каждый сегмент начинается с опорного кадра,<br />
  если качество ухудшается, то просто берется сегмент худшего качества.<br />
HLS — внутри каждого сегмента MPEG2-TS — в итоге большой overhead;<br />
  но apple выпустила fragmented MP4.<br />

Фрагментный стриминг:
- быстрый старт
- задержка на длину сегмента
Для трансляций не очень подходит за счет длительностью задержки.<br />

Требования к протоколу:
- многопоточность
- опциональная гарантия доставки
- пиритизация потоков
- возможность шифрования (всех данных или только критичных)
Чем меньше latency, тем хуже quality.

UDP paces — раздвигает пакеты и контроллирует packet loss.<br />
В TCP есть fast retransmit — нет избыточности, но есть ретрансит период = RTT:

    вместо RTT можно посчитать jitter — как среднюю величину задержек;
    retrans perion = RTT + delta, считают его через jutter.

Если двойной packet drop — проблема со сходимостью:

    решается добавлением избыточности.

Forward error correction — добавляем XOR/Reed-solomon после N пакетов,<br />
то есть избыточность, но блока пакетов.<br />
Negative acknowledgment — когда сервер видит отсутствие пакетов<br />
  в последовательности и посылает NACK инициируя повторную передачу пакета.<br />

Что быстрее FEC + NACK vs FastRetransmit?<br />
  У ОК FastRetransmit работает лучше и быстрее.<br />

Нужно также в своем протоколе учесть, что часть пакетов может быть просто уже неактуальна<br />
и надо их просто дропнуть; это хорошо работает, когда понятно содержимое пакетов.<br />
Идеально работать в размере MTU (Google ставть 1200 байт и остальное сделает IP grafmentation).<br />

Также делают MTU discovery, периодически перепроверяя:<br />
подбирают половинным делением между пакетами с флагом DF (don't fragment).<br />

Шифрование: diffie-helmlan.<br />
Необходимо также понимать, что есть NAT Unbinding.<br />
Только 7% пользователей не смогли работать с этим протоколом,<br />
  помимо этого оставляют возможность работы с WebRTC+RTMP,<br />
  если fallback на мобильных, то MPEG-Dash.<br />
Используют BBR в качестве congestion control алгоритма.


# Хранение петабайтов видео и фото в ОК
OBS — это некоторое blob хранилище, где сервер хранит данные на диске и индекс в памяти.<br />
Но очень большие размеры контента, стало заканчиваться место в ДЦ.<br />

OCS цели:
- хранить дешевле
- хранить надежнее
- обслуживание запросов при выходе из строя ДЦ + диска/сервера
- упрощение эксплуатации

Коды Рида-Соломона позволяют использовать отдельно блоки данных и коды коррекции,<br />
при этом при распределении на 3 ДЦ один из них хранил коды коррекции.<br />
Для замены дисков нужно сделать декодирование — восстановить данные,<br />
при запросе клиента к контенту одного из дисков также нужно сделать декодирование.<br />
Рассматривали, когда блок с данными и корекция в одном ДЦ: неплохая схема с избыточностью<br />
предыдущего варианта + 100%.<br />

Пришли к другому варианту, XOR (вычисляется линейно по ряду,<br />
типа диск1 ДЦ1 xor диск1 ДЦ2 = диск2 ДЦ3) + коды коррекции EvenOdd:
- прямой XOR (также линейно включая предыдущий блок XOR)
- диагональный XOR — второй блок

И в отдельном 3 ДЦ хранился XOR тех блоков, что в других 2-х ДЦ.<br />
Эта схема дает возможность потерять 5 любых дисков в их схеме.<br />

Как записывать данные в таком случае:<br />

    записывают данные на один диск в одном ДЦ, но если диск
    вылетает, то данные теряются; можно писать на несколько
    дисков — но это не очень эффективно.
    В итоге изначально пишут данные в надежный OBS и читают потом в OCS.

Это всё на java.<br />

Воспринимают именно диск как единицу кластера, а не сервер.<br />
У каждого диска есть тред пул, который обрабатывает чтение или запись к диску.<br />
Подключение диска: new RandomAccessfile('/dev/sdc') — работают с диском как файлом.<br />

Как восстанавливают данные: раньше с 10Тб дисками синк был 27 часов,<br />
  сейчас при вылете диска перераспределяют данные на любой другой свободный<br />
  диск/любые другие свободные, поэтому диск восстанавливается за час.<br />
Для одинакового процента заполненности в фоне перемещают данные между дисками.<br />

Во время восстановления latency +30%, как от этого избавиться > CFQ + ioprio_set:
  - клиентский запрос наиболее приоритетен
  - системные — низкий
  - фоновые — наиболее низкий

Что такое индекс:
- Cassandra нода, всё хранится на 3 репликах
- информация «сегмент:смещение:размер»
Но тут нет информации о том, на какой диск и сервер сходить:<br />
для этого сервер сообщает информацию о диске, и сегмент представляет

   сегмент1 -> диск1:хост1
   сегментN -> дискN:хостM

В итоге представление сводится к виду: `дискN:хостM:смещение:размер`<br />
Клиент делает запрос к двум индексам и вычитывает ответ из самой быстрой.


# Как обслужить миллиард пользователей и отдать терабит трафика
Anycast — балансировка трафика с помощью сети, на основании метрик;<br />
пользователь всегда идет на ближайший узел.<br />
TCP synproxy и алгоритмы кеширования ECMP решают проблемы доставки пакетов разным узлам.<br />
Потенциальная балансировка между каналами — out-of-order,<br />
когда tcp-пакеты приходят в обратной последовательности.<br />

*ECMP-балансировка*
- хэш-таблица (через какой интерфейс отправить и пр.)
- хэш-функция > per flow и per packet (последнее редко кто использует)<br />
  xor/crc — вычисление постоянно и предсказуемо, так можем<br />
  обеспечивать consistent hashing (некоторые вендоры могут<br />
  подмешивать дополнительные параметры — чтобы избежать эффект поляризации FIB)

Консистентное хеширование — если вылетает hash bucket, то он просто<br />
перезаписывается и не пересчитываются хэши для остальных bucket-ов,<br />
как в традиционном подходе.<br />

Эффект поляризации FIB:<br />
есть 3 маршрута с разными метриками с одинаковыми весами, и часть роутеров умеет два маршрута;<br />
*часть трафика распределяется так, что некоторые линки остаются недонагруженными*.<br />

Топология Клоза хорошо ложиться для CDN, там трафик можно не баланировать<br />
отдельно с помощью железок, а делать это на уровне маршрутов clos.<br />

ECMP хорошо себя чувствует в сетя spine-leaf.<br />
Эффект поляризации не всегда плохо. Может стабилизировать anycast трафик.<br />
Требуется одинаковый режим балансировки всех устройств.<br />

Ассиметричкая маршрутизация лежит в основе L2 DSR (direct server return),<br />
когда прямой и обратный трафик идут ассиметрично — обратный трафик<br />
идет минуя балансер.<br />
Проблемы:
- масштабируемость L2
- broadcast floading

L3 DSR — отличие в установке ipip туннеля,<br />
на сервере поднимается loopback с этим VIP.<br />

Facebook используется IPVS — проект shiv (Euro python 2016):
- l7 proxy lb
- lb3 IPVS
- ecmp
- dns
- cdn
Google активно разрабатывали maglev — подобно dpdk — основан на консистентном хеше:<br />
  используется для балансировки в GcloudEngine.<br />

Как пользователь попадает на нужный узел CDN:
- gslb — dns mapping: facebook, linkedin
  - важна качественная БД IP
  - кеширование DNS — большая проблема: facebook устанавливаниет<br />
    CNAME с маленьким TTL, для которого IN A также с маленьким TTL
- global anycast


# One-cloud - система управления ДЦ в Одноклассниках
Сервер per задача — низкая утилизация стойкоместа;<br />
следующий этап эволюции 1 сервер = X задач:
- сложная диагностика
- сложное разделение ресурсов
- нет изоляции ресурсов

Контейнеризация — как технология, docker оказался неплохим кандидатом:
+ есть образы ФС
+ многослойность, теги, реестр
+ изоляция: CPU, memory
- сложное размещение контейнеров на серверы — расположение стоек/залов,<br />
  чтобы обеспечить отказоустойчивость
- выделение ресурсов на проект: необходимо сохранение контроля

`client > One-cloud master > One-cloud miniond > docker daemon > linux kernel`

Распределение ресурсов (трафик — тоже ресурс) — выставление квоты<br />
на соответствующем уровне иерархии, то есть по выделенным группам —<br />
иерархическим очередям. Также есть разделение по ролям в пределах группы.<br />

У сервиса есть:
- имя
- манифест, разделение по ресурсам, конфигурациям

Классы задач:
- с малой задержкой: prod (низкое latancy)
  Значительную часть времени такие задачи ждут запроса.
- расчетные (mapreduce, ML и пр.): batch (важна полоса)<br />
  Высокий средний уровень потребления: alloc = cpu [1, *)
- фоновые

Особый профит, когда на одном миньоне оба типа задача — prod + batch, но как это сделать?
— proc cpu = 4

    --cpuset — не подходит, так как ограничивает ядра для задачи

    --cpuquota — лучше подходит
    --cpuperiod

— batch cpu = [1, *)

    Хорошо подходит --cpushares

Теперь нужно приоритезировать prod над batch:<br />
SCHED_OTHER — обычный linux;<br />
SCHED_BATCH — есть штраф за активацию;<br />
SCHED_IDLE — фоновые (< nice -19)<br />
Изнутри миньона выставляется политика планировщика:
- prod: --cpuquota --cpuperiod SCHED_OTHER
- batch: --cpushares --cap-add=SYS_NICE] SCHED_BATCH
- idle: --cpushares --cap-add=SYS_NICE] SCHED_IDLE

Linux QoS:
- traffic control
  - hfsc
  - 2 класса: prod; batch/idle
- modprobe ifb:
  - для QoS входящего трафика
- регулируемая полоса для batch/idleЖ
  - дописали в ОК

Полная изоляция невозможна:
- внутренняя очередь сетевой карты
- выпывается кеш CPU из памяти

В случае отказа миньона нужен service discovery, недостаточно просто пенести.<br />
А нужен ли на самом деле service discovery?
- это ещё одна точка отказа,
- много переписывать для поддержания этой системы,
- балансировок уже и так много.
Как это можно сделать:
- IP для сервиса статичны и следуют за контейнером по сети
- DNS

На миньоне работает bird, который на роут-рефлектор анонсит адреса.<br />
Если миньон отказал, то мастер переносит сервис на другой миньон,<br />
и вновь bird анонсит этот адрес.<br />
В BGP можно указать вес анонса — multi exit descriminator, и при<br />
проблеме с миньоном и переносе этот вес понижается 1_000_000 > 999_999.<br />

В случае переносов задач также есть приоретизация,
есть защита от выполнения опасных задач, таких как
уменьшение количества реплик, смены образа и др.